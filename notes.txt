Neural Net Kit
--------------
versions:
  <branch dev>
  0.1-dev.0
    two samples:
      x basic classifier
      x basic match
    reporting
      image
      histogram
      classification error vs batch
    1st upload to github
    
  0.1-dev.1
    rbm
    one or two samples for rbm
    additional reporting
      regeneration
      mpegs

  0.1-dev.2
    implement remaining dendrites
    sparse ops for theano
    one or two sparse samples
    additional reporting
      sparsity maps

  0.1-dev.3
    serialization
      pickle
      json

  0.1-dev.4
    multiple predecessors/costs

  0.1-dev.5
    error messages and general helpfulness to programmers
      for classifiers (ClassifyInput objectives)
        check that each expected output class < # of inputs to network's output layer

  0.1-dev.6
    optimization

  <branch beta> 0.1-beta.0
    unit tests
    integrated tests

  0.1-beta.1
    ...

  <branch rc> 0.1-rc.0
    stabilize

  0.1-rc.1
    ...

  <branch release> 0.1

  0.1.1 
    maintenance
    
  <branch dev> 0.2-dev.0
    training guards
      overfitting
      divergent learning (learning rate too high)

  0.2-dev.1
    adaptive trainer
      adjust learning rate vs training error
      adjust learning rate vs reconstruction error

  0.2-dev.2
    reporting
      learning rate adjustments
      guard status/output

  <stabilize, release>

  0.3-dev.0
    cd/pcd for decay in backprop

  0.3-dev.1
    reporting
      comparative update rules
    
  0.3-dev.2
    samples

  0.3-dev.3
    optimize
    
  <stabilize, release>

  0.4-dev.0
    grow/prune neurons

  0.4-dev.1
    samples

  <stabilize, release>

  0.5-dev.0
    bias feedback

  0.5-dev.1
    samples

  0.5-dev.3
    optimize
    
  <stabilize, release>

  0.6-dev.0
    weight feedback
    aux rbm

  0.6-dev.1
    samples

  0.6-dev.3
    optimize
    
  <stabilize, release>

  0.7-dev.0
    mixed supervised/unsupervised training

  0.8-dev.0
    audio
    video

  0.9-dev
    setup script
      version system similar to theano
      examine theano's setup script
  


layers
  neuron
  input
  output
  sampling
  processing
  
dendrite types
  complete
  local
  convolution
  sparse

synapse functions
  tanh

training algorithms
  supervised -- backprop
  unsupervised -- rbm

Recurrent Neural Network

Let f be the componentwise nonlinear transfer function of the network.
Let x be the input vector, and let y be the correct output vector.

Define
  v_{0;0} = A_0 * x + b_0
  y_{0;0} = f(v_{0;0})

  v_{0;1} = A_1 * y_{0;0} + b_1
  y_{0;1} = f(v_{0;1})

  e_{0;1} = 1/2 [y_{0;1} - y]^2

Where the A_i are matrices and the b_i are column vectors, and where []^2 represents componentwise
squaring.  The v_{0;i} are the synaptic inputs, the y_{0;i} are the layer activations, and the e_i
are the errors.

Then for t > 0, define

  v_{t;0} = (D_0 * y_{t-1;1} + A_0) * x + B_0 * y_{t-1;1} + b_0
  y_{t;0} = f(v_{t;0})

  v_{t;1} = A_1 * y_{t;0} + b_1
  y_{t;1} = f(v_{t;1})

  e_{t;1} = 1/2 [y_{t;1} - y]^2

Backpropagation Delta Rule

Formulas for de_{0;1}/dA_0, de_{0;1}/db_0, de_{t;1}/dD_0, de_{t;1}/dB_0, de_{t;1}/dA_1, and
de_{t;1}/db_1.

  {de_{t;1}/db_1}_i = {d(f(v_{0;1}))/db_1}_i
                    = f'({v_{0;1}}_i) * {d(v_{0;1})/db_1}_i
                    = {db_1}_i * f'({v_{0;1}}_i)

  {de_{t;1}/dA_1}_i = {d(f(v_{0;1}))/dA_1}_i
                    = f'({v_{0;1}}_i) * {d(v_{0;1})/dA_1}_i
                    = f'({v_{0;1}}_i) * sum over j A_{j, i} * {y_0;0}_j

  e_{t;0} = 
  


Dy + A + E = (Dy + E) + A = 

Dy + E = Fy
E = (F - D)y


-----------------

sparse array for GPU
  use cublas routines by creating new Theano Op




Use CD/PCD neg particles instead of weight decay.

